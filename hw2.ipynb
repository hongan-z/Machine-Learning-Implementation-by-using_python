{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"hw2.ipynb","provenance":[{"file_id":"1P48glDQcGwfflS_DkfMd0e0auz7ELl5h","timestamp":1618554840399},{"file_id":"1afvTWIdasA9t_MSBGOfMhX1pwAM7WIEF","timestamp":1617345072381}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"UKc8K9_57UEz"},"source":["# CS171-EE142 - Spring 2021 - Homework 2\n","\n","# Due: Sunday, May 2, 2021 @ 11:59pm\n","\n","### Maximum points: 100 pts\n","\n","\n","## Submit your solution as a single jupyeter notebook at iLearn.\n"]},{"cell_type":"markdown","metadata":{"id":"JL5tIX4c9z6s"},"source":["\n","### Enter your information below:\n","\n","<div style=\"color: #000000;background-color: #EEEEFF\">\n","    Your Name (submitter):  <br>\n","    Your student ID (submitter):\n","    \n","<b>By submitting this notebook, I assert that the work below is my own work, completed for this course.  Except where explicitly cited, none of the portions of this notebook are duplicated from anyone else's work or my own previous work.</b>\n","</div>\n"]},{"cell_type":"markdown","metadata":{"id":"coxduSVB9gut"},"source":["## Academic Integrity\n","Each assignment should be done  individually. You may discuss general approaches with other students in the class, and ask questions to the TAs, but  you must only submit work that is yours . If you receive help by any external sources (other than the TA and the instructor), you must properly credit those sources, and if the help is significant, the appropriate grade reduction will be applied. If you fail to do so, the instructor and the TAs are obligated to take the appropriate actions outlined at http://conduct.ucr.edu/policies/academicintegrity.html . Please read carefully the UCR academic integrity policies included in the link.\n"]},{"cell_type":"markdown","metadata":{"id":"hsywQWEI8pzj"},"source":["# Overview \n","In this assignment you will implement and test two supervised learning algorithms: linear regression (Problem 1) and logistic regression (Problem 2). \n","\n","For this assignment we will use the functionality of Pandas (https://pandas.pydata.org/), Matplotlib (https://matplotlib.org/), and Numpy (http://www.numpy.org/). You may also find Seaborn (https://seaborn.pydata.org/) useful for some data visualization.\n","\n","If you are asked to **implement** a particular functionality, you should **not** use an existing implementation from the libraries above (or some other library that you may find). When in doubt, please ask. \n","\n","Before you start, make sure you have installed all those packages in your local Jupyter instance\n","\n","\n","## Read *all* cells carefully and answer all parts (both text and missing code)\n","\n","You will complete all the code marked `TODO` and answer descriptive/derivation questions \n","\n"]},{"cell_type":"code","metadata":{"id":"NM7-Cx4H-jTt"},"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sb\n","import random as rand\n","from sklearn.model_selection import train_test_split \n","\n","# make sure you import here everything else you may need"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WHgsgFGF-kNA"},"source":["## Question 1: Linear Regression [70 pts]\n","We will implement linear regression using direct solution and gradient descent. \n","\n","We will first attempt to predict output using a single attribute/feature. Then we will perform linear regression using multiple attributes/features. \n","\n","### Getting data\n","In this assignment we will use the Boston housing dataset. \n","\n","The Boston housing data set was collected in the 1970s to study the relationship between house price and various factors such as the house size, crime rate, socio-economic status, etc.  Since the variables are easy to understand, the data set is ideal for learning basic concepts in machine learning.  The raw data and a complete description of the dataset can be found on the UCI website:\n","\n","https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names\n","https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data\n","\n","or \n","\n","http://www.ccs.neu.edu/home/vip/teach/MLcourse/data/housing_desc.txt\n","\n","I have supplied a list `names` of the column headers.  You will have to set the options in the `read_csv` command to correctly delimit the data in the file and name the columns correctly."]},{"cell_type":"code","metadata":{"id":"PTv71Plf-vZJ"},"source":["names =[\n","    'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', \n","    'AGE',  'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'PRICE'\n","]\n","\n","df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data',\n","                 header=None,delim_whitespace=True,names=names,na_values='?')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eayVrVtUI3zo"},"source":["### Basic Manipulations on the Data\n","\n","What is the shape of the data?  How many attributes are there?  How many samples?\n","Print a statement of the form:\n","\n","    num samples=xxx, num attributes=yy"]},{"cell_type":"markdown","metadata":{"id":"8UULkMygXPpd"},"source":["In order to properly test linear regression, we first need to find a set of correlated variables, so that we use one to predict the other. Consider the following scatterplots:"]},{"cell_type":"code","metadata":{"id":"TxXqO-naI2iG"},"source":["sb.pairplot(df[['RM','LSTAT','PRICE']])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ywadWSHsYEZd"},"source":["Create a response vector `y` with the values in the column `PRICE`.  The vector `y` should be a 1D `numpy.array` structure."]},{"cell_type":"code","metadata":{"id":"p2-zIA5vYOHA"},"source":["# TODO \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l9UHZEggYKd0"},"source":["Use the response vector `y` to find the mean house price in thousands and the fraction of homes that are above $40k. (You may realize this is very cheap.  Prices have gone up a lot since the 1970s!).   Create print statements of the form:\n","\n","    The mean house price is xx.yy thousands of dollars.\n","    Only x.y percent are above $40k."]},{"cell_type":"code","metadata":{"id":"265qk4h2YUrc"},"source":["# TODO \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_zeK-Qoc0TXu"},"source":["### Visualizing the Data\n","\n","Python's `matplotlib` has very good routines for plotting and visualizing data that closely follows the format of MATLAB programs.  You can load the `matplotlib` package with the following commands."]},{"cell_type":"code","metadata":{"collapsed":true,"id":"O5Hn63Id0TXu"},"source":["import matplotlib\n","import matplotlib.pyplot as plt\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A8BLgPQQ0TXv"},"source":["Similar to the `y` vector, create a predictor vector `x` containing the values in the `RM` column, which represents the average number of rooms in each region."]},{"cell_type":"code","metadata":{"collapsed":true,"id":"wvkiFOUP0TXv"},"source":["# TODO\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nZfFM3_z0TXv"},"source":["Create a scatter plot of the price vs. the `RM` attribute.  Make sure your plot has grid lines and label the axes with reasonable labels so that someone else can understand the plot."]},{"cell_type":"code","metadata":{"collapsed":true,"id":"DXkjMSpY0TXv"},"source":["# TODO\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JJ6l2LRjvrEV"},"source":["The number of rooms and price seem to have a linear trend, so let us try to predict price using number of rooms first. "]},{"cell_type":"markdown","metadata":{"id":"G-5e_O4x0TXv"},"source":["### Question 1a. Derivation of a simple linear model for a single feature [10 pts]\n","Suppose we have $N$ pairs of training samples $(x_1,y_1),\\ldots, (x_N,y_N)$, where $x_i \\in \\mathbb{R}$ and $y_i \\in \\mathbb{R}$. \n","\n","We want to perform a linear fit for this 1D data as \n","$$y = wx+b,$$\n","where $w\\in \\mathbb{R}$ and $b\\in \\mathbb{R}$. \n","\n","In the class, we looked at the derivation of optimal value of $w$ when $b=0$. The squared loss function can be written as  $$L(w) = \\sum_{i=1}^N(w x_i -y_i)^2,$$ and the optimal value of $w*$ that minimizes $L(w)$ can be written as $$w^* = (\\sum_{i=1}^N x_i^2)^{-1}(\\sum_{i=1}^N x_i y_i)$$. \n","\n","\n","Now let us include $b$ in our model. Show that the optimal values of $w^*,b^*$ that minimize the loss function \n","$$L(w,b) = \\sum_{i=1}^N(wx_i + b -y_i)^2$$ \n","can be written as \n","$$w^* = (\\sum_i (x_i - \\bar{x})^2)^{-1}(\\sum_i (x_i-\\bar{x})(y_i-\\bar{y}))$$\n","and $$b^* = \\bar{y} - w^*\\bar{x},$$\n","where $\\bar{x} = \\frac{1}{N}\\sum_i x_i, \\bar{y} = \\frac{1}{N}\\sum_i y_i$ are mean values of $x_i,y_i$, respectively. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"JXgc6X6nnCxf"},"source":["**TODO: Your derivation goes here.**\n","\n","\n","\n","\n","*   *Hint. Set the partial derivative of $L(w,b)$ with respect to $w$ and $b$ to zero.*\n","*   Type using latex commands and explain your steps\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"v6wR2wx9nHRG"},"source":["### Question 1b. Fitting a linear model using a single feature [10 pts] \n","\n","Next we will write a function to perform a linear fit. Use the formulae above to compute the parameters $w,b$ in the linear model $y = wx + b$."]},{"cell_type":"code","metadata":{"collapsed":true,"id":"HA-PyCGR0TXv"},"source":["def fit_linear(x,y):\n","    \"\"\"\n","    Given vectors of data points (x,y), performs a fit for the linear model:\n","       yhat = w*x + b, \n","    The function returns w and b\n","    \"\"\"\n","    # TODO complete the code below\n","    \n","    return w, b"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GzO9Ehhg0TXw"},"source":["Using the function `fit_linear` above, print the values `w`, `b` for the linear model of price vs. number of rooms."]},{"cell_type":"code","metadata":{"collapsed":true,"id":"LxoaYYOz0TXw"},"source":["# TODO\n","w, b,_ = fit_linear(x,y)\n","print('w = {0:5.1f}, b = {1:5.1f}'.format(w,b))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bAj_BRcerz97"},"source":["Does the price increase or decrease with the number of rooms? \n","\n","* *Your answer here*"]},{"cell_type":"markdown","metadata":{"id":"F71MzBOx0TXw"},"source":["Replot the scatter plot above, but now with the regression line.  You can create the regression line by creating points `xp` from say min(x) to max(x), computing the linear predicted values `yp` on those points and plotting `yp` vs. `xp` on top of the above plot."]},{"cell_type":"code","metadata":{"collapsed":true,"id":"rf9qJ_qY0TXw"},"source":["# TODO\n","# Points on the regression line\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v_23tqcUdIfx"},"source":["### Question 1c. Linear regression with multiple features/attributes [20 pts]\n","One possible way to try to improve the fit is to use multiple variables at the same time."]},{"cell_type":"markdown","metadata":{"id":"bpsZK-mYWyFS"},"source":["In this exercise, the target variable will be the `PRICE`.  We will use multiple attributes of the house to predict the price.  \n","\n","The names of all the data attributes are given in variable `names`. \n","* We can get the list of names of the columns from `df.columns.tolist()`.  \n","* Remove the last items from the list using indexing."]},{"cell_type":"code","metadata":{"id":"FYwmOGtxwuLA"},"source":["xnames = names[:-1]\n","print(names[:-1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mLDuJkzNxUQu"},"source":["Let us use `CRIM`, `RM`, and `LSTAT` to predict `PRICE`. \n","\n","Get the data matrix `X` with three features (`CRIM`, `RM`, `LSTAT`) and target vector `y` from the dataframe `df`.  \n","\n","Recall that to get the items from a dataframe, you can use syntax such as\n","\n","    s = np.array(df['RM'])  \n","        \n","which gets the data in the column `RM` and puts it into an array `s`.  You can also get multiple columns with syntax like\n","\n","    X12 = np.array(df['CRIM', 'ZN'])  \n"]},{"cell_type":"code","metadata":{"id":"Zp9Gk4O8FSty"},"source":["# TODO\n","#    X = ...\n","#    y = ...\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jfHwrw4mzuS6"},"source":["**Linear regression in scikit-learn**\n","\n","To fit the linear model, we could create a regression object and then fit the training data with regression object.\n","\n","```\n","from sklearn import linear_model\n","regr = linear_model.LinearRegression()\n","regr.fit(X_train,y_train)\n","```\n","\n","You can see the coefficients as\n","```\n","regr.intercept_\n","regr.coef_\n","```\n","\n","We can predict output for any data as \n","\n","    y_pred = regr.predict(X)\n","\n","**Instead of taking this approach, we will implement the regression function directly.**"]},{"cell_type":"markdown","metadata":{"id":"HC3dIR9Ns3Ny"},"source":["**Linear regression by solving least-squares problem (direct solution)**\n","\n","Suppose we have $N$ pairs of training samples $(x_1,y_1),\\ldots, (x_N,y_N)$, where $\\mathbf{x}_i \\in \\mathbb{R}^d$ and $y_i \\in \\mathbb{R}$. \n","\n","We want to perform a linear fit over all the data features as \n","$$y = \\mathbf{\\tilde w}^T\\mathbf{x}+b,$$\n","where $\\mathbf{\\tilde w}\\in \\mathbb{R}^d$ and $b\\in \\mathbb{R}$. \n","\n","We saw in the class that we can write all the training data as a linear system \n","$$ \\begin{bmatrix} y_1 \\\\ \\vdots \\\\ y_N \\end{bmatrix} = \\begin{bmatrix} - & \\mathbf{x}_1^T & - \\\\ \n","& \\vdots & \\\\\n","- & \\mathbf{x}_N^T& - \\end{bmatrix} \\mathbf{\\tilde w} + b, $$\n","which can be written as \n","$$ \\begin{bmatrix} y_1 \\\\ \\vdots \\\\ y_N \\end{bmatrix} = \\begin{bmatrix} 1 & \\mathbf{x}_1^T \\\\ \n","\\vdots & \\vdots \\\\\n","1 & \\mathbf{x}_N^T\\end{bmatrix} \\begin{bmatrix} b \\\\ \\mathbf{\\tilde w} \\end{bmatrix}.$$\n","\n","Let us write this system of linear equations in a compact form as \n","\\begin{equation} \n","\\mathbf{y} = \\mathbf{X}\\mathbf{w}, \n","\\end{equation} \n","where $\\mathbf{X}$ is an $N \\times d+1$ matrix whose first column is all ones and $\\mathbf{w}$ is a vector of length $d+1$ whose first term is the constant and rest of them are the coefficients of the linear model. \n","\n","The least-squares problem for the system above can be written as \n","$$\\text{minimize}\\; \\|\\mathbf{y} - \\mathbf{X}\\mathbf{w}\\|_2^2$$\n","for which the closed form solution can be written as \n","$$\\mathbf{w} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}.$$"]},{"cell_type":"markdown","metadata":{"id":"rid0H49X6fpi"},"source":["**Append ones to the data matrix**\n","\n","To compute the coefficients $\\mathbf{\\tilde w}$, we first append a vector of ones to the data matrix.  This can be performed with the `ones` command and `hstack`.  Note that after we do this, `X` will have one more column than before. "]},{"cell_type":"code","metadata":{"id":"sod1xEK67AzN"},"source":["# TODO  \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0Pche190FSty"},"source":["**Split the Data into Training and Test**\n","\n","Split the data into training and test.  Use 30% for test and 70% for training.  You can do the splitting manually or use the `sklearn` package `train_test_split`.   Store the training data in `Xtr,ytr` and test data in `Xts,yts`.\n"]},{"cell_type":"code","metadata":{"id":"8t88nZxdFSty"},"source":["from sklearn.model_selection import train_test_split\n","\n","# TODO\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V27_oEaX75i-"},"source":["Now let us compute the coefficients $\\mathbf{w}$ using `Xtr,ytr` via the direct matrix inverse: $$\\mathbf{w} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}.$$\n","\n","You may use `np.linalg.inv` to compute the inverse. For a small problem like this, it makes no difference.  But, in general, using a matrix inverse like this is *much* slower computationally than using functions such as `lstsq` method or the `LinearRegression` class.  In real world, you will never solve a least squares problem like this. "]},{"cell_type":"code","metadata":{"id":"uwBWVVY_8gUN"},"source":["# TODO\n","# compute w using the direct solution equation \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QCWqasJL9I6h"},"source":["Compute the predicted values `yhat_tr` on the training data and print the average square loss value on the training data."]},{"cell_type":"code","metadata":{"id":"MXtUFymP9M0P"},"source":["# TODO \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IGcEO0ggFStz"},"source":["Create a scatter plot of the actual vs. predicted values of `y` on the training data."]},{"cell_type":"code","metadata":{"id":"-r__jPyk-Em-"},"source":["# TODO\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1HCzNYlm-UxT"},"source":["Compute the predicted values `yhat_ts` on the test data and print the average square loss value on the test data."]},{"cell_type":"code","metadata":{"id":"mUKr8Dx2-UxU"},"source":["# TODO \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W7i50wvq-CWO"},"source":["Create a scatter plot of the actual vs. predicted values of `y` on the test data."]},{"cell_type":"code","metadata":{"id":"q8dnwG94FStz"},"source":["# TODO\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B75oOA9NCMwg"},"source":["### Question 1d: Gradient descent for linear regression [20 pts]\n","Finally, we will implement the gradient descent version of linear regression.\n","\n","In particular, the function implemented should follow the following format:\n","```python\n","def linear_regression_gd(X,y,learning_rate = 0.00001,max_iter=10000,tol=pow(10,-5)):\n","```\n","Where `X` is the same data matrix used above (with ones column appended), `y` is the variable to be predicted, `learning_rate` is the learning rate used ($\\alpha$ in the slides), `max_iter` defines the maximum number of iterations that gradient descent is allowed to run, and `tol` is defining the tolerance for convergence (which we'll discuss next).\n","\n","The return values for the above function should be (at the least) 1) `w` which are the regression parameters, 2) `all_cost` which is an array where each position contains the value of the objective function $L(\\mathbf{w})$ for a given iteration, 3) `iters` which counts how many iterations did the algorithm need in order to converge to a solution.\n","\n","Gradient descent is an iterative algorithm; it keeps updating the variables until a convergence criterion is met. In our case, our convergence criterion is whichever of the following two criteria happens first:\n","\n","- The maximum number of iterations is met\n","- The relative improvement in the cost is not greater than the tolerance we have specified. For this criterion, you may use the following snippet into your code:\n","```python\n","np.absolute(all_cost[it] - all_cost[it-1])/all_cost[it-1] <= tol\n","```\n","\n","Gradient can be computed as $$\\nabla_\\mathbf{w}L = \\mathbf{X}^T(\\mathbf{X}\\mathbf{w} - \\mathbf{y}).$$\n","\n","Estimate will be updated as $ \\mathbf{w} \\gets \\mathbf{w} - \\alpha \\nabla_\\mathbf{w}L$ at every iteration. \n","\n","**Note that the $\\mathbf{w}$ in this derivation includes the constant term and $\\mathbf{X}$ is a matrix that has ones colum"]},{"cell_type":"code","metadata":{"id":"rI3bPa4CY7XW"},"source":["# TODO \n","# Implement gradient descent for linear regression \n","\n","def compute_cost(X,w,y):\n","    # your code for the loss function goes here \n","\n","    return L;\n","\n","def linear_regression_gd(X,y,learning_rate = 0.00001,max_iter=10000,tol=pow(10,-5)):\n","    # your code goes here \n","    \n","    return w,all_cost,iters;"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dN2e6tsuY8Nq"},"source":["### Question 1e: Convergence plots [10 pts]\n","After implementing gradient descent for linear regression, we would like to test that indeed our algorithm converges to a solution. In order see this, we are going to look at the value of the objective/loss function $L(\\mathbf{w})$ as a function of the number of iterations, and ideally, what we would like to see is $L(\\mathbf{w})$ drops as we run more iterations, and eventually it stabilizes. \n","\n","The learning rate plays a big role in how fast our algorithm converges: a larger learning rate means that the algorithm is making faster strides to the solution, whereas a smaller learning rate implies slower steps. In this question we are going to test two different values for the learning rate:\n","- 0.00001\n","- 0.000001\n","\n","while keeping the default values for the max number of iterations and the tolerance.\n","\n","\n","- Plot the two convergence plots (cost vs. iterations) [5]\n","\n","- What do you observe? [5]\n","\n","<b>Important</b>: In reality, when we are running gradient descent, we should be checking convergence based on the <i>validation</i> error (i.e., we would have to split our training set into e.g., 70/30 training'/validation subsets, use the new training set to calculate the gradient descent updates and evaluate the error both on the training set and the validation set, and as soon as the validation loss stops improving, we stop training. <b>In order to keep things simple, in this assignment we are only looking at the training loss</b>, but as long as you have a function \n","```python\n","def compute_cost(X,w,y):\n","```\n","that calculates the loss for a given X, y, and set of parameters you have, you can always compute it on the validation portion of X and y (that are <b>not</b> used for the updates).  "]},{"cell_type":"code","metadata":{"id":"1vbaLoa7ZB_8"},"source":["# TODO \n","# test gradient descent with step size 0.00001\n","# test gradient descent with step size 0.000001\n","\n","(w, all_cost,iters) = linear_regression_gd(Xtr,ytr,learning_rate = 0.00001,max_iter = 1000, tol=pow(10,-6))  \n","plt.figure(0)\n","plt.semilogy(all_cost[0:iters])    \n","plt.grid()\n","plt.xlabel('Iteration')\n","plt.ylabel('Training loss')  \n","\n","# complete the rest"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KRU51zCtZAtP"},"source":["Observations: \n","\n","1. \n","1.  "]},{"cell_type":"markdown","metadata":{"id":"_7FVGGI_aqre"},"source":["### Question 2. Logistic regression [30 pts]\n","\n","In this question, we will plot the logistic function and perform logistic regression. We will use the breast cancer data set.  This data set is described here:\n","\n","https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin\n","\n","Each sample is a collection of features that were manually recorded by a physician upon inspecting a sample of cells from fine needle aspiration.  The goal is to detect if the cells are benign or malignant.  \n","\n","We could use the `sklearn` built-in `LogisticRegression` class to find the weights for the logistic regression problem.  The `fit` routine in that class has an *optimizer* to select the weights to best match the data.  To understand how that optimizer works, in this problem, we will build a very simple gradient descent optimizer from scratch.  "]},{"cell_type":"markdown","metadata":{"id":"EHaz8CZBfUVr"},"source":["### Loading and visualizing the Breast Cancer Data\n","\n","We load the data from the UCI site and remove the missing values."]},{"cell_type":"code","metadata":{"id":"4RdmLqolffmw"},"source":["names = ['id','thick','size_unif','shape_unif','marg','cell_size','bare',\n","         'chrom','normal','mit','class']\n","df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/' +\n","                 'breast-cancer-wisconsin/breast-cancer-wisconsin.data',\n","                names=names,na_values='?',header=None)\n","df = df.dropna()\n","df.head(6)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3EEPMsFmfpqS"},"source":["After loading the data, we can create a scatter plot of the data labeling the class values with different colors.  We will pick two of the features.  "]},{"cell_type":"code","metadata":{"id":"rXSf9mKtfwbF"},"source":["# Get the response.  Convert to a zero-one indicator \n","yraw = np.array(df['class'])\n","BEN_VAL = 2   # value in the 'class' label for benign samples\n","MAL_VAL = 4   # value in the 'class' label for malignant samples\n","y = (yraw == MAL_VAL).astype(int)\n","Iben = (y==0)\n","Imal = (y==1)\n","\n","# Get two predictors\n","xnames =['size_unif','marg'] \n","X = np.array(df[xnames])\n","\n","# Create the scatter plot\n","plt.plot(X[Imal,0],X[Imal,1],'r.')\n","plt.plot(X[Iben,0],X[Iben,1],'g.')\n","plt.xlabel(xnames[0], fontsize=16)\n","plt.ylabel(xnames[1], fontsize=16)\n","plt.ylim(0,14)\n","plt.legend(['malign','benign'],loc='upper right')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UY5fLe90gCnU"},"source":["The above plot is not informative, since many of the points are on top of one another.  Thus, we cannot see the relative frequency of points.  "]},{"cell_type":"markdown","metadata":{"id":"1RaZ9ToBgD6-"},"source":["### Logistic function\n","\n","We will build a binary classifier using *logistic regression*.  In logistic regression, we do not just output an estimate of the class label.  Instead, we ouput a *probability*, an estimate of how likely the sample is one class or the other.  That is our output is a number from 0 to 1 representing the likelihood:\n","$$\n","    P(y = 1|x)\n","$$\n","which is our estimate of the probability that the sample is one class (in this case, a malignant sample) based on the features in `x`.  This is sometimes called a *soft classifier*.  \n","\n","In logistic regression, we assume that likelihood is of the form\n","$$\n","    P(y=1|x) = g(z),  \\quad z = w(1)x(1) + \\cdots + w(d)x(d) + b = \\mathbf{w}^T\\mathbf{x}+b,  \n","$$\n","where $w(1),\\ldots,w(d),b$ are the classifier weights and $g(z)$ is the so-called *logistic* or *sigmoid* function:\n","$$\n","    g(z) = \\frac{1}{1+e^{-z}}.\n","$$\n","\n","To understand the logistic function, suppose $x$ is a scalar and samples $y$ are drawn with $P(y=1|x) = f(w x+b)$ for some $w$ and $b$.  We plot these samples for different $w,b$."]},{"cell_type":"code","metadata":{"id":"shwwuK9Ogv7D"},"source":["N = 100\n","xm = 20\n","ws = np.array([0.5,1,2,10])\n","bs = np.array([0, 5, -5])\n","wplot = ws.size\n","bplot = bs.size\n","iplot = 0\n","for b in bs: \n","  for w in ws:\n","    iplot += 1\n","    x  = np.random.uniform(-xm,xm,N)\n","  \n","    py = 1/(1+np.exp(-w*x-b))\n"," \n","    yp = np.array(np.random.rand(N) < py) # hard label for random points\n","    xp = np.linspace(-xm,xm,100) \n","    pyp = 1/(1+np.exp(-w*xp-b)) # soft label (probability) for the points\n","\n","    plt.subplot(bplot,wplot,iplot)\n","\n","    plt.scatter(x,yp,c=yp,edgecolors='none',marker='+')\n","    plt.plot(xp,pyp,'b-')\n","    plt.axis([-xm,xm,-0.1,1.1])\n","    plt.grid() \n","    if ((iplot%4)!=1):\n","        plt.yticks([])\n","    plt.xticks([-20,-10,0,10,20])\n","    plt.title('w={0:.1f}, b={1:.1f}'.format(w,b))\n","\n","    plt.subplots_adjust(top=1.5, bottom=0.2, hspace=0.5, wspace=0.2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kyhRv_1hhOj3"},"source":["We see that $\\sigma(wx+b)$ represents the probability that $y=1$.  The function $\\sigma(wx) > 0.5$ for $x>0$ meaning the samples are more likely to be $y=1$.  Similarly, for $x<0$, the samples are more likely to be $y=0$.  The scaling $w$ determines how fast that transition is and $b$ influences the transition point.  "]},{"cell_type":"markdown","metadata":{"id":"Dr2_5WR2cY9p"},"source":["### Fitting the Logistic Model on Two  Variables\n","\n","We will fit the logistic model on the two variables `size_unif` and `marg` that we were looking at earlier."]},{"cell_type":"code","metadata":{"id":"WQPUSMlTcY9q"},"source":["# load data \n","xnames =['size_unif','marg'] \n","X = np.array(df[xnames])\n","print(X.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zvgB0iCCcY9q"},"source":["Next we split the data into training and test"]},{"cell_type":"code","metadata":{"id":"2oC7pomCcY9q"},"source":["# Split into training and test\n","from sklearn.model_selection import train_test_split\n","Xtr, Xts, ytr, yts = train_test_split(X,y, test_size=0.30)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qr4ru1WYcY9v"},"source":["**Logistic regression in scikit-learn**\n","\n","The actual fitting is easy with the `sklearn` package.  The parameter `C` \n","states the level of inverse regularization strength with higher values meaning less regularization. Right now, we will select a high value to minimally regularize the estimate.\n","\n","We can also measure the accuracy on the test data. You should get an accuracy around 90%. "]},{"cell_type":"code","metadata":{"id":"lKGhsUYfzUx3"},"source":["from sklearn import datasets, linear_model, preprocessing\n","reg = linear_model.LogisticRegression(C=1e5)\n","reg.fit(Xtr, ytr)\n","\n","print(reg.coef_)\n","print(reg.intercept_)\n","\n","yhat = reg.predict(Xts)\n","acc = np.mean(yhat == yts)\n","print(\"Accuracy on test data = %f\" % acc)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dOhsTYG6zXDk"},"source":["**Instead of taking this approach, we will implement the regression function using gradient descent.**"]},{"cell_type":"markdown","metadata":{"id":"KCTbi1wTcbIn"},"source":["### Question 2a. Gradient descent for logistic regression [20 pts]\n","In the class we saw that the weight vector can be found by minimizing the negative log likelihood over $N$ training samples.  The negative log likelihood is called the *loss* function.  For the logistic regression problem, the loss function simplifies to\n","\n","$$L(\\mathbf{w}) = - \\sum_{i=1}^N y_i \\log \\sigma(\\mathbf{w}^T\\mathbf{x}_i+b) + (1-y_i)\\log [1-\\sigma(\\mathbf{w}^T\\mathbf{x}_i+b)].$$\n","\n","Gradient can be computed as $$\\nabla_\\mathbf{w}L = \\sum_{i=1}^N(\\sigma(\\mathbf{w}^T\\mathbf{x}_i)-y_i)\\mathbf{x}_i ,~~~ \\nabla_b L = \\sum_{i=1}^N(\\sigma(\\mathbf{w}^T\\mathbf{x}_i)-y_i).$$\n","\n","\n","We can update $\\mathbf{w},b$ at every iteration as  \n","$$ \\mathbf{w} \\gets \\mathbf{w} - \\alpha \\nabla_\\mathbf{w}L, \\\\ b \\gets b - \\alpha \\nabla_b L.$$ \n","\n","**Note that we could also append the constant term in $\\mathbf{w}$ and append 1 to every $\\mathbf{x}_i$ accordingly, but we kept them separate in the expressions above.**\n","\n"," "]},{"cell_type":"markdown","metadata":{"id":"QNa10nUiS5ru"},"source":["**Gradient descent function implementation** \n","\n","We will use this loss function and gradient to implement a gradient descent-based method for logistic regression.\n","\n","Recall that training a logistic function means finding a weight vector `w` for the classification rule:\n","\n","    P(y=1|x,w) = 1/(1+\\exp(-z)), z = w[0]+w[1]*x[1] + ... + w[d]x[d]\n","    \n","The function implemented should follow the following format:\n","```python\n","def logistic_regression_gd(X,y,learning_rate = 0.001,max_iter=1000,tol=pow(10,-5)):\n","```\n","Where `X` is the training data feature(s), `y` is the variable to be predicted, `learning_rate` is the learning rate used ($\\alpha$ in the slides), `max_iter` defines the maximum number of iterations that gradient descent is allowed to run, and `tol` is defining the tolerance for convergence (which we'll discuss next).\n","\n","The return values for the above function should be (at the least) 1) `w` which are the regression parameters, 2) `all_cost` which is an array where each position contains the value of the objective function $L(\\mathbf{w})$ for a given iteration, 3) `iters` which counts how many iterations did the algorithm need in order to converge to a solution.\n","\n","Gradient descent is an iterative algorithm; it keeps updating the variables until a convergence criterion is met. In our case, our convergence criterion is whichever of the following two criteria happens first:\n","\n","- The maximum number of iterations is met\n","- The relative improvement in the cost is not greater than the tolerance we have specified. For this criterion, you may use the following snippet into your code:\n","```python\n","np.absolute(all_cost[it] - all_cost[it-1])/all_cost[it-1] <= tol\n","```"]},{"cell_type":"code","metadata":{"id":"SRDxzsZk8ck_"},"source":["# TODO \n","# Your code for logistic regression via gradient descent goes here \n","\n","def compute_cost(X,w,y):\n","    # your code for the loss function goes here \n","\n","    return L;\n","\n","def logistic_regression_gd(X,y,learning_rate = 0.00001,max_iter=1000,tol=pow(10,-5)):\n","    # your code goes here \n","\n","    return w,all_cost,iters;"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"08r6_M-98clK"},"source":["### Question 2b: Convergence plots and test accuracy [10 pts]\n","\n","After implementing gradient descent for logistic regression, we would like to test that indeed our algorithm converges to a solution. In order see this, we are going to look at the value of the objective/loss function $L(\\mathbf{w})$ as a function of the number of iterations, and ideally, what we would like to see is $L(\\mathbf{w})$ drops as we run more iterations, and eventually it stabilizes. \n","\n","The learning rate plays a big role in how fast our algorithm converges: a larger learning rate means that the algorithm is making faster strides to the solution, whereas a smaller learning rate implies slower steps. In this question we are going to test two different values for the learning rate:\n","- 0.001\n","- 0.00001\n","\n","while keeping the default values for the max number of iterations and the tolerance.\n","\n","\n","- Plot the two convergence plots (cost vs. iterations)\n","- Calculate the accuracy of classifier on the test data `Xts` \n","- What do you observe? \n"]},{"cell_type":"markdown","metadata":{"id":"2XWJPz_uU1zT"},"source":["**Calculate accuracy of your classifier on test data**\n","\n","To calculate the accuracy of our classifier on the test data, we can create a predict method. \n","\n","Implement a function `predict(X,w)` that provides you label 1 if $\\mathbf{w}^T\\mathbf{x} + b > 0$ and 0 otherwise.  "]},{"cell_type":"code","metadata":{"id":"uFyHcnVFVD4r"},"source":["# TODO \n","# Predict on test samples and measure accuracy\n","def predict(X,w):\n","  # your code goes here \n","\n","  return yhat"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P7KAVMGV8clL"},"source":["# TODO \n","# test gradient descent with step size 0.001\n","# test gradient descent with step size 0.00001\n"," \n","(w, all_cost,iters) = logistic_regression_gd(Xtr,ytr,learning_rate = 0.001,max_iter = 1000, tol=pow(10,-6))  \n","plt.semilogy(all_cost[0:iters])    \n","plt.grid()\n","plt.xlabel('Iteration')\n","plt.ylabel('Training loss') \n","\n","yhat = predict(Xts,w)\n","acc = np.mean(yhat == yts)\n","print(\"Test accuracy = %f\" % acc)\n","\n","# complete the rest "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zu5HpI_-8clL"},"source":["Observations: \n","\n","1. \n","1.  "]}]}