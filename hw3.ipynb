{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UKc8K9_57UEz"
   },
   "source": [
    "# CS171-EE142 - Spring 2021 - Homework 3\n",
    "\n",
    "# Due: Friday, May 21, 2021 @ 11:59pm\n",
    "\n",
    "### Maximum points: 80 pts\n",
    "\n",
    "\n",
    "## Submit your solution as a single jupyeter notebook at iLearn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JL5tIX4c9z6s"
   },
   "source": [
    "\n",
    "### Enter your information below:\n",
    "\n",
    "<div style=\"color: #000000;background-color: #EEEEFF\">\n",
    "    Your Name (submitter):  <br>\n",
    "    Your student ID (submitter):\n",
    "    \n",
    "<b>By submitting this notebook, I assert that the work below is my own work, completed for this course.  Except where explicitly cited, none of the portions of this notebook are duplicated from anyone else's work or my own previous work.</b>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "coxduSVB9gut"
   },
   "source": [
    "## Academic Integrity\n",
    "Each assignment should be done  individually. You may discuss general approaches with other students in the class, and ask questions to the TAs, but  you must only submit work that is yours . If you receive help by any external sources (other than the TA and the instructor), you must properly credit those sources, and if the help is significant, the appropriate grade reduction will be applied. If you fail to do so, the instructor and the TAs are obligated to take the appropriate actions outlined at http://conduct.ucr.edu/policies/academicintegrity.html . Please read carefully the UCR academic integrity policies included in the link.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hsywQWEI8pzj"
   },
   "source": [
    "# Overview \n",
    "In this assignment you will implement a two-layer neural network. You will implement the loss functions, gradients, optimizers to train the network and test its performance on MNIST dataset. \n",
    "\n",
    "For this assignment we will use the functionality of Pandas (https://pandas.pydata.org/), Matplotlib (https://matplotlib.org/), and Numpy (http://www.numpy.org/). \n",
    "\n",
    "If you are asked to **implement** a particular functionality, you should **not** use an existing implementation from the libraries above (or some other library that you may find). When in doubt, please ask. \n",
    "\n",
    "Before you start, make sure you have installed all those packages in your local Jupyter instance\n",
    "\n",
    "## Read *all* cells carefully and answer all parts (both text and missing code)\n",
    "\n",
    "You will complete all the code marked `TODO` and answer descriptive/derivation questions \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "3d6urIY6Ci2D"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import random as rand\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "# make sure you import here everything else you may need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9vBsJizSAN5C"
   },
   "source": [
    "### Load MNIST Dataset \n",
    "\n",
    "For this assignment, we will use [MNIST](https://en.wikipedia.org/wiki/MNIST_database) handwritten digits data set. The dataset consists 10 handwritten digits (0,1,...,9). It is a widely used dataset to demonstrate simple image classification problem.\n",
    "\n",
    "MNIST dataset is publicly available from different sources. We will be using MNIST from Keras package. If you do not have Keras installed, you can find the installation guide [here](https://www.tutorialspoint.com/keras/keras_installation.htm). \n",
    "\n",
    "In short, you need to run ```conda install -c anaconda keras``` or ```pip install keras```\n",
    "\n",
    "The training data consists of 60000 images of size $28 \\times 28$ pixels; the test data consists of 10000 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Jdo3YbSzAN5D",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomRotation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-9d31727f0f70>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Training data shape:'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Test data shape:'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomRotation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     raise ImportError(\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[1;34m'Keras requires TensorFlow 2.2 or higher. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         'Install TensorFlow via `pip install tensorflow`')\n",
      "\u001b[1;31mImportError\u001b[0m: Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "print('Training data shape:',x_train.shape)\n",
    "print('Test data shape:',x_test.shape)\n",
    "\n",
    "n_img=10\n",
    "plt.figure(figsize=(n_img*2,2))\n",
    "plt.gray()\n",
    "for i in range(n_img):\n",
    "    plt.subplot(1,n_img,i+1)\n",
    "    plt.imshow(x_train[i])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uw4N4norAN5D"
   },
   "source": [
    "We will be vectorizing the training and test images. So, the size of each vector will be 784."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VOIMLKYtAN5E"
   },
   "outputs": [],
   "source": [
    "x_train=x_train.reshape(x_train.shape[0],-1)\n",
    "x_test=x_test.reshape(x_test.shape[0],-1)\n",
    "\n",
    "print('Training data shape after reshaping:',x_train.shape)\n",
    "print('Test data shape after reshaping::',x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iHVQmrbIAN4-"
   },
   "source": [
    "## Question 1a: Binary classification using neural network [45 pts]\n",
    "\n",
    "We will start with classification of images for two different digits using a two-layer network with a cross entropy loss. \n",
    "\n",
    "In the next question, we will extend the same architecture to multi-class classification. \n",
    "\n",
    "Pick any two digits out of ten for our classification (say 5 and 8), which we will assign label \"0\" or \"1\". \n",
    "\n",
    "Pick same number of images from each class for training and create arrays for input and output (say 1000). \n",
    "\n",
    "```\n",
    "# train_x -- N x 784 array of training input\n",
    "# train_y -- N x 1 array of binary labels \n",
    "```  \n",
    "\n",
    "If you use 1000 images from each class N = 2000. You can increase the number of training samples if you like. It is just a suggestion. \n",
    "\n",
    "\n",
    "We also need to transpose the dimension of the data so that their size becomes $784\\times N$. It will be helpful to feed it to our model based on our notations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fir2-wZWAN5E"
   },
   "outputs": [],
   "source": [
    "# Pick your own digits\n",
    "label1=3\n",
    "label2=8\n",
    "\n",
    "N = 1000\n",
    "\n",
    "# Train data\n",
    "x1=x_train[y_train==label1]\n",
    "print(x1.shape)\n",
    "x1 = x1[:N]\n",
    "y1=np.zeros(len(x1))\n",
    "\n",
    "x2=x_train[y_train==label2]\n",
    "x2 = x2[:N]\n",
    "y2=np.ones(len(x2))\n",
    "\n",
    "x=np.concatenate((x1,x2),axis=0)\n",
    "y=np.concatenate((y1,y2),axis=0)\n",
    "\n",
    "train_x = x; \n",
    "train_y = y;\n",
    "print(\"Training data shape:\",train_x.shape)\n",
    "\n",
    "\n",
    "# Test data\n",
    "x1=x_test[y_test==label1]\n",
    "y1=np.zeros(len(x1))\n",
    "\n",
    "x2=x_test[y_test==label2]\n",
    "y2=np.ones(len(x2))\n",
    "\n",
    "test_x=np.concatenate((x1,x2),axis=0)\n",
    "test_y=np.concatenate((y1,y2),axis=0)\n",
    "print(\"Test data shape:\",test_x.shape)\n",
    "\n",
    "# reshape data \n",
    "train_x=train_x.T\n",
    "test_x=test_x.T\n",
    "print(\"Training data shape:\",train_x.shape)\n",
    "print(\"Test data shape:\",test_x.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HdeO0YieAN5E"
   },
   "source": [
    "### Network Architecture\n",
    "\n",
    "We will be using a two layer neural network in our experiment. The input layer will have 784 nodes, the hidden layer will have 256 nodes and the output layer will have 1 node. Each node will have $\\textit{sigmoid}$ activation function.\n",
    "\n",
    "The equations for feedforward operation will be the following:\n",
    "\n",
    "$$\\mathbf{z}^{(1)}=W^{(1)} \\mathbf{x}+ \\mathbf{b}^{(1)}\\\\\\mathbf{y}^{(1)}=\\varphi(\\mathbf{z}^{(1)})\\\\\\mathbf{z}^{(2)}=W^{(2)} \\mathbf{x}+ \\mathbf{b}^{(2)} \\\\\\mathbf{y}^{(2)}=\\varphi(\\mathbf{z}^{(2)})$$\n",
    "\n",
    "where $\\mathbf{x}\\in \\mathbb{R}^{784}$ is the input layer, $\\mathbf{y}^{(1)}\\in \\mathbb{R}^{256}$ is the hidden layer, $\\mathbf{y}^{(2)} \\in \\mathbb{R}$ is the output layer, $W^{(1)}\\in \\mathbb{R}^{256\\times 784}$ is the first layer weights, $W^{(2)}\\in \\mathbb{R}^{1\\times 256}$ is the second layer weights, $\\mathbf{b}^{(1)}\\in \\mathbb{R}^{256}$ is the first layer bias, $\\mathbf{b}^{(2)}\\in \\mathbb{R}$ is the second layer bias, $\\varphi(\\cdot)$ is the activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XWw3zn2_AN5F"
   },
   "source": [
    "### Network initialization [5 pts]\n",
    "\n",
    "We initialize the weights for $W^{(1)}$ and $W^{(2)}$ with random values drawn from normal distribution with zero mean and 0.01 standard deviation. We will initialize bias vectors $\\mathbf{b}^{(1)}$ and $\\mathbf{b^{(2)}}$ with zero values. \n",
    "\n",
    "We can fix the seed for random initialization for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5k52OUaoAN5F"
   },
   "outputs": [],
   "source": [
    "def TwoLayerNetwork(layer_dims=[784,256,1]):\n",
    "    # TODO \n",
    "    # Your code goes here\n",
    "\n",
    "    # Fix the seed\n",
    "    np.random.seed(3)\n",
    "\n",
    "    ... \n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0xLDm_wkAN5F"
   },
   "source": [
    "### Sigmoid activation function \n",
    "Now we will write the sigmoid activation function as \n",
    "\n",
    "$$ \\varphi(z) = \\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "Note that derivative of __sigmoid__ is $\\varphi'(z) = \\varphi(z) (1-\\varphi(z))$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_CPxvM0UAN5F"
   },
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    # TODO \n",
    "    # Write your function\n",
    "    \n",
    "    \n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GGuGMhzeAN5G"
   },
   "source": [
    "### Cross entropy loss function [5 pts]\n",
    "We will minimize the binary cross entropy loss function. You will use the true labels and predicted labels of a batch of N samples. \n",
    "\n",
    "Binary crossentropy loss for $i^{th}$ sample can be written as \n",
    "\n",
    "$$L_i = -y_i \\log y^{(2)}_i- (1-y_i) \\log (1-y^{(2)}_i)$$\n",
    "\n",
    "where $y_i$ is the true label. We can find the average loss for a batch of N samples as $Loss=\\frac{1}{N}\\sum_{i=1}^{N} L_i$.\n",
    "\n",
    "Note that the gradient of the cross entropy loss w.r.t. the output is \n",
    "\n",
    "$$ \\nabla_{y^{(2)}} L_i = -\\frac{y_i}{y_i^{(2)}} + \\frac{1-y_i}{1-y_i^{(2)}} = \\frac{y_i^{(2)}-y_i}{y_i^{(2)}(1-y_i^{(2)})}.$$\n",
    "\n",
    "We can also show that $$\\delta^{(2)} = \\nabla_{\\mathbf{z}^{(2)}} L_i  = \\nabla_{y^{(2)}} L_i \\odot \\varphi'(\\mathbf{z})= y_i^{(2)}-y_i.$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l7KkfTXeAN5G"
   },
   "outputs": [],
   "source": [
    "def CrossEntropyLoss(Y_true,Y2):\n",
    "    # TODO \n",
    "    # Write your code here\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EnFFFVP1AN5F"
   },
   "source": [
    "### Forward propagation  [5 pts]\n",
    "Next, we will write the code for the forward pass for two layer network. Each layer consists of an affine function (fully-connected layer) followed by an activation function. You wil also return the intermediate results ($\\mathbf{x}, \\mathbf{z}^{(1)}, \\mathbf{y}^{(1)}, \\mathbf{z}^{(2)}$) in addition to final output ($\\mathbf{y}^{(2)}$). You will need the intermediate outputs for the backpropagation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oBHoC1_TAN5F"
   },
   "outputs": [],
   "source": [
    "def forward(X, params):\n",
    "    \n",
    "    # TODO \n",
    "    # Write your codes here\n",
    "\n",
    "    # X -- 784 x N array \n",
    "    # params -- \n",
    "      # W1 -- 256 x 784 matrix\n",
    "      # b1 -- 256 x 1 vector\n",
    "      # W2 -- 1 x 256 matrix\n",
    "      # b2 -- 1 x 1 scalar \n",
    "    # Y2 -- 1 x N output \n",
    "    \n",
    "\n",
    "    return Y2, intermediate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I4ZM2NLAAN5G"
   },
   "source": [
    "### Backpropagration step [10 pts]\n",
    "Now we will implement the backpropagation step for the two layer neural network. \n",
    "\n",
    "You will need the following derivatives for $l = 1,2$: \n",
    "\n",
    "$$\\frac{\\partial Loss}{\\partial w_{ij}^l} = \\frac{\\partial Loss}{\\partial \\mathbf{y}_i^l}\\frac{\\partial \\mathbf{y}_i^l}{\\partial \\mathbf{z}_i^l}\\frac{\\partial \\mathbf{z}_i^l}{\\partial w_{ij}^l}$$ \n",
    "\n",
    "We saw that we can write the gradient of Loss with respect to $W^{(l)}$ as\n",
    "\n",
    "$$\\nabla_{W^{(l)}} Loss = \\delta^{(l)} \\mathbf{y}^{(l-1)T},$$  \n",
    "\n",
    "where \n",
    "$$\\delta^{(l)} = \\nabla_{\\mathbf{z}^{(l)}} Loss = \\nabla_{\\mathbf{y}^{(l)}} Loss \\odot \\varphi'(\\mathbf{z}^{(l)}).$$ \n",
    "\n",
    "Please refer to the slides and lectures for more details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ODa4OXAqAN5G"
   },
   "outputs": [],
   "source": [
    "def backward(Y_true, Y2, intermediate,params):\n",
    "    \n",
    "    # Inputs: \n",
    "      # Y_true -- 1 x N true labels\n",
    "      # Y2 -- 1 x N output of the last layer\n",
    "      # intermediate -- X, Z1, Y1, Z2 \n",
    "      # params -- W1, b1, W2, b2 \n",
    "    \n",
    "    # Outputs: \n",
    "      # grads -- [grad_W1, grad_b1, grad_W2, grad_b2]\n",
    "    \n",
    "    # TODO \n",
    "    # Write your codes here\n",
    "\n",
    "     \n",
    "          \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GlHu8oIaAN5G"
   },
   "source": [
    "### Optimizer [5 pts]\n",
    "We will use a standard gradient descent-based optimizer to minimize the loss function. You have already implemented gradient descent in HW2. You may have to adjust learning rate that provides you best training/validation performance. In this exercise, we are not using validation data; in practice, you should use it to tune your hyperparameters such as learning rate, network architecture etc. \n",
    "\n",
    "You can use same learning rate for all weights in this assignment. \n",
    "\n",
    "You should update $W^1, \\mathbf{b}^1, W^2, \\mathbf{b}^2$ as \n",
    "$$ W^1 \\gets W^1 - \\alpha \\nabla_{W^1} Loss $$\n",
    "$$ \\mathbf{b}^1 \\gets \\mathbf{b}^1 - \\alpha \\nabla_{\\mathbf{b}^1} Loss $$ \n",
    "$$ W^2 \\gets W^2 - \\alpha \\nabla_{W^2} Loss $$ \n",
    "$$ \\mathbf{b}^2 \\gets \\mathbf{b}^2 - \\alpha \\nabla_{\\mathbf{b}^2} Loss $$ \n",
    "$\\alpha$ is the learning rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "noqJ8U_PAN5G"
   },
   "outputs": [],
   "source": [
    "def GD(params, grads, learning_rate):\n",
    "    \n",
    "    # updated params = old params - learning rate * gradient of Loss computed at old params\n",
    "    # TODO \n",
    "    # Write your codes here\n",
    "     \n",
    "     \n",
    "        \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adhSifEGAN5G"
   },
   "source": [
    "### Train the Model [5 pts]\n",
    "We will train the model using the functions we wrote above. \n",
    "\n",
    "First, we specify the number of nodes in the layers, number of epochs and learning rate. Then we initialize the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lKwAatEkAN5H"
   },
   "outputs": [],
   "source": [
    "layer_dims=[train_x.shape[0],256,1]\n",
    "epochs=100\n",
    "lr=0.01\n",
    "\n",
    "params = TwoLayerNetwork(layer_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kdi_L3xYAN5H"
   },
   "source": [
    "Then we train the network for the number of epochs specified above. In every epoch, we will do the following:\n",
    "1. Calculate the forward pass to get estimated labels.\n",
    "2. Use the estimated labels calculate loss. We will be recording loss for every epoch.\n",
    "3. Use backpropagation to calculate gradients.\n",
    "4. Use gradient descent to update the weights and biases.\n",
    "\n",
    "You should store the loss value after every epoch in an array ```loss_history```  and print the loss value after every few epochs (say 20). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QtEPxEleAN5H"
   },
   "outputs": [],
   "source": [
    "# TODO \n",
    "# Write your codes here\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2-7VVZ6iAN5H"
   },
   "source": [
    "Now we will plot the recorded loss values vs epochs. We will observe the training loss decreasing with the epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uu8zTsABAN5H"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ZlLv1NbAN5H"
   },
   "source": [
    "### Evaluation on test data [5 pts]\n",
    "\n",
    "Now we will be evaluating the accuracy we get from the trained model. We feed training data and test data to the forward model along with the trained parameters. \n",
    "\n",
    "Note that, we need to covert the output probability of the forward pass to binary labels before evaluating accuracy. Since the model provides the posterior probability $p(y = 1 | x)$ in range [0,1]. We can binarize them using 0.5 as a theshold (i.e. if $y_i^{(2)}\\geq 0.5$, $y_i^{(2)} \\gets 1$ otherwise  $y_i^{(2)} \\gets 0$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HKZ7BSZjAN5I"
   },
   "outputs": [],
   "source": [
    "# TODO \n",
    " \n",
    "print(\"Training accuracy:\",???)\n",
    "\n",
    "print(\"Test accuracy:\",???)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ba-laCd-4OkR"
   },
   "source": [
    "### Visualize some of the correct/miscalassified images [5 pts]\n",
    "\n",
    "Now we will look at some images from training and test sets that were misclassified. \n",
    "\n",
    "Training set. \n",
    "Pick 5 images from each class that are correcly and incorreclty classified. \n",
    "True/False Positive/Negatives\n",
    "\n",
    "Test set. \n",
    "Pick 5 images from each class that are correcly and incorreclty classified. \n",
    "True/False Positive/Negatives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1O5zZTCn4N18"
   },
   "outputs": [],
   "source": [
    "# TODO \n",
    "# Training set\n",
    "print(\"Training set examples for true/false positive/negative\")\n",
    "Y_hat, caches = forward(train_x, params)\n",
    "\n",
    "# your code goes here...\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jym9VlP-_0rd"
   },
   "outputs": [],
   "source": [
    "# Test set\n",
    "print(\"Test set examples for true/false positive/negative\")\n",
    "Y_hat, caches = forward(test_x, params)\n",
    "\n",
    "# your code goes here... \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XBEj5ST72SG6"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zoli2hpzp2gy"
   },
   "source": [
    "## Question 1b. Multiclass classification [35 pts] \n",
    "\n",
    "Now we will build a classifier to separate all the digits. For this purpose, we will only change the last layer and the loss. \n",
    "\n",
    "\n",
    "Instead of using a single output, we will provide 10 outputs; and instead of using a binary cross entropy loss, we will use mutli-class cross entropy loss. \n",
    "\n",
    "In multinomal logistic regression (aka softmax regression), we define the posterior probability of label $y \\in \\{0,\\ldots, K-1\\}$ as \n",
    "\n",
    "\n",
    "$$p(y = c | \\mathbf{x}) = \\frac{\\exp(\\mathbf{w}_c^T\\mathbf{x})}{\\sum_{k=1}^K \\exp(\\mathbf{w}_k^T\\mathbf{x})} = \\mathbf{p}_c.$$ \n",
    "\n",
    "In other words, last layer of the network provides a probability vector $\\mathbf{p} \\in \\mathbb{R}^K$, such that each $0 \\le \\mathbf{p}_c \\le 1$ and $\\sum_c \\mathbf{p}_c = 1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vxGPKVAUBO-w"
   },
   "source": [
    "### Softmax function [5 pts]\n",
    "\n",
    "Let us first define the softmax function, which is a multinomal extension of the sigmoid function that maps a vector of length $K$ to a probability vector. \n",
    "\n",
    "We can define ```softmax``` function on a vector $\\mathbf{z} \\in \\mathbb{R}^K$ as $\\mathbf{p} = \\text{softmax}(\\mathbf{z})$: \n",
    "\n",
    "$$\\mathbf{p}_c(\\mathbf{z}) = \\frac{\\exp(\\mathbf{z}_c)}{\\sum_{k=1}^K \\exp(\\mathbf{z}_k)}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wezeelLcBSCo"
   },
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    # TODO  \n",
    "    # your code goes here... \n",
    "\n",
    "    \n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9J53N9_wWMUf"
   },
   "source": [
    "We have to note that the numerical range of floating point numbers in numpy is limited. For `float64` the upper bound is $10^{308}$. For exponential, its not difficult to overshoot that limit, in which case python returns `nan`.\n",
    "\n",
    "To make our softmax function numerically stable, we simply normalize the values in the vector, by multiplying the numerator and denominator with a constant `C` as\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{p}_c  &= \\frac{\\exp(\\mathbf{z}_c)}{\\sum_{k=1}^K \\exp(\\mathbf{z}_k)} \\\\\n",
    "& = \\frac{C\\exp(\\mathbf{z}_c)}{C\\sum_{k=1}^K \\exp(\\mathbf{z}_k)}\\\\\n",
    "& = \\frac{\\exp(\\mathbf{z}_c + \\log C)}{C\\sum_{k=1}^K \\exp(\\mathbf{z}_k + \\log C)}.\n",
    "\\end{align*}\n",
    "\n",
    "We can choose an arbitrary value for `log(C)` term, but generally `log(C) = −max(z)` is chosen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JzmyZdoqXO_v"
   },
   "outputs": [],
   "source": [
    "def stable_softmax(Z): \n",
    "    # TODO (this is optional)\n",
    "    # your code goes here  \n",
    "\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tSGHPdySbpbj"
   },
   "source": [
    "### Derivative of the softmax function\n",
    "\n",
    "We can show that the derivative of the __softmax__ function with respect to any input can be written as \n",
    "\n",
    "$$ \\frac{\\partial \\mathbf{p}_i}{\\partial \\mathbf{z}_j} = \\begin{cases} \\mathbf{p}_i(1-\\mathbf{p}_j) & i = j \\\\ \\mathbf{p}_i (-\\mathbf{p}_j) & i \\ne j. \\end{cases}$$\n",
    "\n",
    "[More info here](https://deepnotes.io/softmax-crossentropy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C0qcy_JAArEo"
   },
   "source": [
    "### Multiclass cross entropy loss function [5 pts]\n",
    "\n",
    "We will minimize the cross entropy loss. You will use the true labels and predicted labels of a batch of N samples. \n",
    "\n",
    "The multi-class cross entropy loss for $i^{th}$ sample can be written as \n",
    "$$L_i = -\\sum_c \\mathbf{1}(y_i = c) \\log \\mathbf{p}_c $$\n",
    "where $y_i$ is the true label and \n",
    "\n",
    "$$\\mathbf{1}(y_i = c) = \\begin{cases} 1 & y_i =c \\\\ 0 & \\text{otherwise} \\end{cases}$$ \n",
    "is an indicator function. \n",
    "\n",
    "We can find the average loss for a batch of N samples as $Loss=\\frac{1}{N}\\sum_{i=1}^{N} L_i$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tebVzE2SAoTB"
   },
   "outputs": [],
   "source": [
    "def MultiClassCrossEntropyLoss(Y_true,probs):\n",
    "  \n",
    "  # TODO \n",
    "  # Write your code here\n",
    "\n",
    "  # probs -- K x N array\n",
    "  # Y_true -- 1 x N array \n",
    "  # loss --  sum L_i over N samples \n",
    "   \n",
    "\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tbRPrVLpjFIo"
   },
   "source": [
    "### Derivative of the cross entropy loss \n",
    "\n",
    "Let us assume that $\\mathbf{p} = \\text{softmax}(\\mathbf{z})$. \n",
    "\n",
    "Note that the derivative of the loss w.r.t. $\\mathbf{p}_j$ can be written as \n",
    "$$\\frac{\\partial L_i }{\\partial \\mathbf{p}_j} = \\begin{cases} -1/\\mathbf{p}_j & j = y_i \\\\ 0 & j \\ne y_i \\end{cases}. $$\n",
    "\n",
    "Note that we can use _total derivative_ to compute the derivative of the loss for $i$th sample w.r.t. $j$th entry in $\\mathbf{z}$ as\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L_i}{\\partial \\mathbf{z}_j} = \\sum_c \\frac{\\partial L_i}{\\partial \\mathbf{p}_c}\\frac{\\partial \\mathbf{p}_c}{\\partial \\mathbf{z}_j}.\n",
    "\\end{align*}\n",
    "\n",
    "From our discussion above, we know that the $\\frac{\\partial L_i}{\\partial \\mathbf{p}_c} = 0$ if $ c \\ne y_i$. \n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L_i}{\\partial \\mathbf{z}_j} &= -\\frac{1}{\\mathbf{p}_c} \\frac{\\partial \\mathbf{p}_c}{\\partial \\mathbf{z}_j} \\\\\n",
    "& = \\begin{cases} \\mathbf{p}_j - 1 & j = y_i \\\\ \\mathbf{p}_j & j \\ne y_i. \\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "Therefore, $$\\delta^{(2)} = \\nabla_{\\mathbf{z}^{(2)}} L_i = \\mathbf{p} - \\mathbf{1}_{y_i}.$$\n",
    "\n",
    "where $\\mathbf{1}_{y_i}$ is a __one-hot vector__ that has length $K$ and is zero everywhere except 1 at index same as $y_i$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "462ORdiFLdz2"
   },
   "source": [
    "### Training data\n",
    "\n",
    "Let us pick training data for multi-class classification. \n",
    "\n",
    "Pick same number of images from each class for training and create arrays for input and output. \n",
    "\n",
    "```\n",
    "# train_x -- N x 784 array of training input\n",
    "# train_y -- N x 1 array of binary labels \n",
    "```  \n",
    "\n",
    "If you use 1000 images from each class N = 10000. You can increase the number of training samples if you like. You may also use unequal number of images in each class. \n",
    "\n",
    "We also need to transpose the dimension of the data so that their size becomes $784\\times N$. It will be helpful to feed it to our model based on our notations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BZ6Z-yPgL3xm"
   },
   "outputs": [],
   "source": [
    "# Pick training samples \n",
    "\n",
    "n = 1000\n",
    "\n",
    "# Train data\n",
    "x = np.zeros((0,784))\n",
    "y = np.zeros((0))\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "for label in range(10):\n",
    "  # print(label)\n",
    "  x1=x_train[y_train==label]  \n",
    "  x1 = x1[:n]\n",
    "  y1=y_train[y_train==label]\n",
    "  y1 = y1[:n]\n",
    "  \n",
    "  x=np.concatenate((x,x1),axis=0)\n",
    "  y=np.concatenate((y,y1),axis=0)\n",
    "\n",
    "\n",
    "train_x = x; \n",
    "train_y = y;\n",
    "print(\"Training data shape:\",train_x.shape)\n",
    "\n",
    "\n",
    "# Test data\n",
    "test_x=x_test\n",
    "test_y=y_test \n",
    "print(\"Test data shape:\",test_x.shape)\n",
    "\n",
    "train_x=train_x.T\n",
    "test_x=test_x.T\n",
    "print(\"Training data shape:\",train_x.shape) \n",
    "print(\"Test data shape:\",test_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xoXHKji6L3xo"
   },
   "source": [
    "### Network Architecture\n",
    "\n",
    "We will be using a two layer neural network in our experiment. The input layer has 784 nodes, the hidden layer will have 256 nodes and the output layer will have 10 nodes. First layer will have __sigmoid__ activation and second layer will have __softmax__ activation.\n",
    "\n",
    "The equations for feedforward operation will be as follows.\n",
    "\n",
    "$$\\mathbf{z}^{(1)}=W^{(1)} \\mathbf{x}+ \\mathbf{b}^{(1)}\\\\\\mathbf{y}^{(1)}=\\text{sigmoid}(\\mathbf{z}^{(1)})\\\\\\mathbf{z}^{(2)}=W^{(2)} \\mathbf{x}+ \\mathbf{b}^{(2)} \\\\\\mathbf{p} = \\mathbf{y}^{(2)}=\\text{softmax}(\\mathbf{z}^{(2)})$$\n",
    "\n",
    "where $\\mathbf{x}\\in \\mathbb{R}^{784}$ is the input layer, $\\mathbf{y}^{(1)}\\in \\mathbb{R}^{256}$ is the hidden layer, $\\mathbf{y}^{(2)} \\in \\mathbb{R}$ is the output layer, $W^{(1)}\\in \\mathbb{R}^{256\\times 784}$ is the first layer weights, $W^{(2)}\\in \\mathbb{R}^{10\\times 256}$ is the second layer weights, $\\mathbf{b}^{(1)}\\in \\mathbb{R}^{256}$ is the first layer bias, $\\mathbf{b}^{(2)}\\in \\mathbb{R}^{10}$ is the second layer bias vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Fqsp1BvL3xp"
   },
   "source": [
    "### Network initialization [5 pts]\n",
    "\n",
    "We initialize the weights for $W^{(1)}$ and $W^{(2)}$ with random values drawn from normal distribution with zero mean and 0.01 standard deviation. We will initialize bias vectors $\\mathbf{b}^{(1)}$ and $\\mathbf{b^{(2)}}$ with zero values. \n",
    "\n",
    "We can fix the seed for random initialization for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nu-3CQlRL3xp"
   },
   "outputs": [],
   "source": [
    "def TwoLayerNetwork(layer_dims=[784,256,10]):\n",
    "    # TODO \n",
    "    # Your code goes here\n",
    "\n",
    "    # Fix the seed\n",
    "    np.random.seed(3)\n",
    "    \n",
    "    #Initialize the weights\n",
    "     \n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "av7l9tOKUuAN"
   },
   "source": [
    "### Forward propagation \n",
    "Next, we will write the code for the forward pass for two layer network. Each layer consists of an affine function (fully-connected layer) followed by an activation function. You wil also return the intermediate results ($\\mathbf{x}, \\mathbf{z}^{(1)}, \\mathbf{y}^{(1)}, \\mathbf{z}^{(2)}$) in addition to final output ($\\mathbf{y}^{(2)}$). You will need the intermediate outputs for the backpropagation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kLSoTwFkUuAO"
   },
   "outputs": [],
   "source": [
    "def forward(X, params):\n",
    "    \n",
    "    # TODO \n",
    "    # Write your codes here\n",
    "\n",
    "    # X -- 784 x N array \n",
    "    # params -- \n",
    "      # W1 -- 256 x 784 matrix\n",
    "      # b1 -- 256 x 10 vector\n",
    "      # W2 -- 10 x 256 matrix\n",
    "      # b2 -- 10 x 1 scalar \n",
    "    # probs -- 10 x N output \n",
    " \n",
    "\n",
    "    return probs, intermediate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3QjvXHIUuAO"
   },
   "source": [
    "### Backpropagration step [10 pts]\n",
    "\n",
    "Now we will implement the backpropagation step for the two layer neural network using softmax layer and loss function. \n",
    "\n",
    "You will need the following derivatives for $l = 1,2$: \n",
    "\n",
    "$$\\frac{\\partial Loss}{\\partial w_{ij}^l} = \\frac{\\partial Loss}{\\partial \\mathbf{y}_i^l}\\frac{\\partial \\mathbf{y}_i^l}{\\partial \\mathbf{z}_i^l}\\frac{\\partial \\mathbf{z}_i^l}{\\partial w_{ij}^l}$$ \n",
    "\n",
    "We saw that we can write the gradient of Loss with respect to $W^{(l)}$ as\n",
    "\n",
    "$$\\nabla_{W^{(l)}} Loss = \\delta^{(l)} \\mathbf{y}^{(l-1)T},$$  \n",
    "\n",
    "where \n",
    "$$\\delta^{(l)} = \\nabla_{\\mathbf{z}^{(l)}} Loss = \\nabla_{\\mathbf{y}^{(l)}} Loss \\odot \\varphi'(\\mathbf{z}^{(l)}).$$ \n",
    "\n",
    "We saw above that for an $i$th sample, $\\delta^{(2)} = \\nabla_{\\mathbf{z}^{(2)}} L_i = \\mathbf{p} - \\mathbf{1}_{y_i},$ where $\\mathbf{1}_{y_i}$ is a __one-hot vector__ that has length $K$ and is zero everywhere except 1 at index same as $y_i$ and $\\mathbf{p}$ is the outpu probability vector for the $i$th sample. \n",
    "\n",
    "To compute $\\delta^{(2)}$ for $N$ samples, you just need to compute $\\delta^{(2)} = \\sum_i \\nabla_{\\mathbf{z}^{(2)}} L_i.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T0hPvx0SUuAP"
   },
   "outputs": [],
   "source": [
    "def backward(Y_true, probs, intermediate,params):\n",
    "    \n",
    "    # Inputs: \n",
    "      # Y_true -- true labels\n",
    "      # probs -- 10 x N output of the last layer\n",
    "      # intermediate -- X, Z1, Y1, Z2 \n",
    "      # params -- W1, b1, W2, b2 \n",
    "    \n",
    "    # Outputs: \n",
    "      # grads -- [grad_W1, grad_b1, grad_W2, grad_b2]\n",
    "    \n",
    "    # TODO \n",
    "    # Write your codes here\n",
    "    \n",
    "          \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1DTxxcDTvVQD"
   },
   "source": [
    "### Train the model [5 pts]\n",
    "We will use the forward and backward functions defined above with the same optimizer defined in the previous question to train our multi-class classificaiton model. \n",
    "\n",
    "We will specify the number of nodes in the layers, number of epochs and learning rate and initialize the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xmhkhUisUuAQ"
   },
   "outputs": [],
   "source": [
    "layer_dims=[train_x.shape[0],256,10]\n",
    "epochs=100\n",
    "lr=0.03\n",
    "\n",
    "params = TwoLayerNetwork(layer_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9DZAfG5QUuAQ"
   },
   "source": [
    "Then we train the network for the number of epochs specified above. In every epoch, we will do the following:\n",
    "1. Calculate the forward pass to get estimated labels.\n",
    "2. Use the estimated labels calculate loss. We will be recording loss for every epoch.\n",
    "3. Use backpropagation to calculate gradients.\n",
    "4. Use gradient descent to update the weights and biases.\n",
    "\n",
    "You should store the loss value after every epoch in an array ```loss_history```  and print the loss value after every few epochs (say 20). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P1zSP6g8UuAQ"
   },
   "outputs": [],
   "source": [
    "# TODO \n",
    "# Write your codes here\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mAtKFEjTUuAQ"
   },
   "source": [
    "Now we will plot the recorded loss values vs epochs. We will observe the training loss decreasing with the epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hw14IunrUuAQ"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pf4fN1udUuAR"
   },
   "source": [
    "### Evaluation on test data [5 pts]\n",
    "\n",
    "Now we will be evaluating the accuracy we get from the trained model. We feed training data and test data to the forward model along with the trained parameters. \n",
    "\n",
    "Note that, we need to convert the (probability) output of the forward pass into labels before evaluating accuracy. We can assign label based on the maximum probability. \n",
    "\n",
    "We assign estimated labels $\\hat{y}_i = \\arg \\max_c  \\mathbf{p}_c $ for every probility vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KAhOtQigUuAR"
   },
   "outputs": [],
   "source": [
    "# TODO  \n",
    "\n",
    "print(\"Training accuracy:\",???)\n",
    "\n",
    "print(\"Test accuracy:\",???)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iLjwg5wjMcyB"
   },
   "source": [
    "### Visualize some of the correct/miscalassified images [optional]\n",
    "\n",
    "Now we will look at some images from training and test sets that were misclassified. \n",
    "\n",
    "Training set. \n",
    "Pick example from each class that are correcly and incorreclty classified. \n",
    "True/False Positive/Negatives\n",
    "\n",
    "Test set. \n",
    "Pick examples from each class that are correcly and incorreclty classified. \n",
    "True/False Positive/Negatives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R-NtpDz-Mkub"
   },
   "outputs": [],
   "source": [
    "# TODO \n",
    "# Your code goes here ...\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw3.ipynb",
   "provenance": [
    {
     "file_id": "1CiNzn6dgUMRk1HfxFPG_xZz7VTttMcuO",
     "timestamp": 1620515925951
    },
    {
     "file_id": "1Z6lYMpb4bygcDckti4rImGrfWPEzu4GJ",
     "timestamp": 1620515473539
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
